{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from random import choice\n",
    "from tkinter import _flatten\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allentity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all entity\n",
    "def getcandidate(filepath):\n",
    "    entity = []\n",
    "    with open(filepath) as f:\n",
    "        diclist = json.load(f)['entries']\n",
    "        for dic in diclist:\n",
    "            dic = list(dic.values())[0]\n",
    "            for ops in dic[\"modifiedtripleset\"]:\n",
    "                entity.append(' '.join(ops['subject'].split('_')).replace('\"', '').lower())\n",
    "                entity.append(' '.join(ops['object'].split('_')).replace('\"', '').lower())\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change entity\n",
    "def change(wd):\n",
    "    if random.random() < 1:\n",
    "        return choice(allentity), True\n",
    "    else:\n",
    "        return wd, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sentence and mark where changed\n",
    "def makenoisy(gold, datadiclist, datastr):\n",
    "    noisytext = [gold]\n",
    "    # sum = 0\n",
    "    entitys = []\n",
    "    # get all entity in sentence \n",
    "    for datadic in datadiclist:\n",
    "        entitys.append(' '.join(datadic['subject'].lower().split('_')))\n",
    "        entitys.append(' '.join(datadic['object'].lower().split('_')))\n",
    "    # split by entity\n",
    "    for entity in entitys:\n",
    "        noisytext = [x.split(' '+entity+' ') for x in noisytext]\n",
    "        z = []\n",
    "        for x in noisytext:\n",
    "            tmp = [x[0].strip()]\n",
    "            for y in x[1:]:\n",
    "                tmp.append(entity)\n",
    "                tmp.append(y.strip())\n",
    "            z.append(tmp)\n",
    "        noisytext = [x for x in list(_flatten(z)) if x]\n",
    "\n",
    "    changedid = []\n",
    "    originid = []\n",
    "    retext = []\n",
    "\n",
    "    # mark where changed\n",
    "    # wd is not a word but words\n",
    "    for i, wd in enumerate(noisytext):\n",
    "        if wd in entitys:\n",
    "            nwd, wh = change(wd)\n",
    "            if wh:\n",
    "                changedid.append(i)\n",
    "                originid.append(entitys.index(wd))\n",
    "                wd = nwd\n",
    "        retext.append(wd)\n",
    "    \n",
    "    prelen = 0\n",
    "    errortextpos = ''\n",
    "    corredatapos = ''\n",
    "\n",
    "    # from index to position\n",
    "    for i, wd in enumerate(retext):\n",
    "        if i in changedid:\n",
    "            errortextpos+='\\t'+str(prelen)+' '+str(prelen+len(wd.split(' ')))\n",
    "        prelen += len(wd.split(' '))\n",
    "    # data position\n",
    "    for i in originid:\n",
    "        lis = datastr.split('> '+entitys[i])\n",
    "        corredatapos+='\\t'+str(len(lis[0].strip().split(' ')))+' ' + str(len(lis[0].strip().split(' '))+len(entitys[i].split(' ')))\n",
    "    return ' '.join(retext), errortextpos[1:], corredatapos[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change file, just some form\n",
    "def makepseudo(filepath):\n",
    "    with open(filepath) as f:\n",
    "        fsrcdata = open('../pseudo/' + filepath.split('_')[-1].split('.')[0] + '.src.data', 'w')\n",
    "        fsrctext = open('../pseudo/' + filepath.split('_')[-1].split('.')[0] + '.src.text', 'w')\n",
    "        ftgttext = open('../pseudo/' + filepath.split('_')[-1].split('.')[0] + '.tgt.text', 'w')\n",
    "        ftgtdatapos = open('../pseudo/' + filepath.split('_')[-1].split('.')[0] + '.tgt.datapos', 'w')\n",
    "        ftgttextpos = open('../pseudo/' + filepath.split('_')[-1].split('.')[0] + '.tgt.textpos', 'w')\n",
    "\n",
    "        diclist = json.load(f)['entries']\n",
    "        for dic in tqdm(diclist):\n",
    "            dic = list(dic.values())[0]\n",
    "            dic.pop('originaltriplesets')\n",
    "            textlist = dic.pop('lexicalisations')\n",
    "            textlist = [x['lex'] for x in textlist]\n",
    "            datastr = '<category> '+dic['category']+' <type> '+dic['shape_type']+' <sep>'\n",
    "            for datadic in dic[\"modifiedtripleset\"]:\n",
    "                datastr += ' <s> '+' '.join(datadic['subject'].split('_')).lower()+' <p> '+datadic['property']+' <o> '+' '.join(datadic['object'].split('_')).lower()\n",
    "            \n",
    "            datastr = datastr + ' ' + '<end>'\n",
    "            for text in textlist:\n",
    "                text = text.lower()\n",
    "                ftgttext.write(text+'\\n')\n",
    "                noisytext, errortextpos, corredatapos = makenoisy(text, dic[\"modifiedtripleset\"], datastr)\n",
    "                fsrctext.write(noisytext + '\\n')\n",
    "                fsrcdata.write(datastr + '\\n')\n",
    "                ftgtdatapos.write(corredatapos + '\\n')\n",
    "                ftgttextpos.write(errortextpos + '\\n')\n",
    "        ftgttext.close() \n",
    "        fsrcdata.close()\n",
    "        fsrctext.close()\n",
    "        ftgttextpos.close()\n",
    "        ftgtdatapos.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "allentity=getcandidate('webnlg_release_v2.1_dev.json') + getcandidate('webnlg_release_v2.1_test.json') + getcandidate('webnlg_release_v2.1_train.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1600/1600 [00:00<00:00, 7469.18it/s]\n",
      "100%|██████████| 1619/1619 [00:00<00:00, 7669.73it/s]\n",
      "100%|██████████| 12876/12876 [00:01<00:00, 8824.58it/s]\n"
     ]
    }
   ],
   "source": [
    "makepseudo('webnlg_release_v2.1_test.json')\n",
    "makepseudo('webnlg_release_v2.1_dev.json')\n",
    "makepseudo('webnlg_release_v2.1_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "93588"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(allentity)"
   ]
  },
  {
   "source": [
    "# to classify"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = ['train', 'dev', 'test']\n",
    "for ss in sss:\n",
    "    with open('../pseudo/' + ss + '.tgt.textpos') as f:\n",
    "        tof = open('../pseudo/' + ss + '.tgt.textlabel', 'w')\n",
    "        ftext = open('../pseudo/' + ss + '.src.text')\n",
    "        for line, tl in zip(f, ftext):\n",
    "            lis = [int(a) for b in [x.split(' ') for x in line.strip().split('\\t')] for a in b if a]\n",
    "            res = ['0']*len(tl.strip().split(' '))\n",
    "            for i in range(0, len(lis), 2):\n",
    "                res[lis[i]:lis[i+1]] = ['2']*(lis[i+1]-lis[i])\n",
    "                res[lis[i]] = '1'\n",
    "            tof.write(' '.join(res)+'\\n')\n",
    "        tof.close()\n"
   ]
  },
  {
   "source": [
    "# check"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('../pseudo/dev.src.text')\n",
    "f2 = open('../pseudo/dev.tgt.textlabel')\n",
    "for l1, l2 in zip(f1, f2):\n",
    "    if (len(l1.strip().split(' ')) != len(l2.strip().split(' '))):\n",
    "        print(1)"
   ]
  },
  {
   "source": [
    "# data label"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = ['train', 'dev', 'test']\n",
    "for ss in sss:\n",
    "    with open('../pseudo/'+ss+'.src.data') as f:\n",
    "        tof = open('../pseudo/'+ss+'.tgt.data', 'w')\n",
    "        for ii, line in enumerate(f):\n",
    "            line = [x for x in line.strip().split(' ') if x]\n",
    "            tof.write(' '.join(line)+'\\n')\n",
    "        tof.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = ['train', 'dev', 'test']\n",
    "for ss in sss:\n",
    "    with open('../pseudo/'+ss+'.tgt.data') as f:\n",
    "        tof = open('../pseudo/'+ss+'.tgt.datalabel', 'w')\n",
    "        for ii, line in enumerate(f):\n",
    "            forth, line = line.strip().split('<sep>')\n",
    "            wdlist = [x for x in line.strip().split(' ') if x]\n",
    "            forth = [x for x in forth.strip().split(' ') if x]\n",
    "            lblist = ['0']*len(wdlist)\n",
    "            for i, s in enumerate(wdlist):\n",
    "                try:\n",
    "                    if s[0] != '<' and s[-1] != '>':\n",
    "                        if lblist[i-1] == '0':\n",
    "                            lblist[i] = '1'\n",
    "                        else:\n",
    "                            lblist[i] = '2'\n",
    "                except IndexError:\n",
    "                    print(ii)\n",
    "                    print(wdlist)\n",
    "                    print(s)\n",
    "                    raise\n",
    "            lblist = ['0']*(len(forth)+1) + lblist\n",
    "            tof.write(' '.join(lblist)+'\\n')\n",
    "        tof.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sss = ['train', 'dev', 'test']\n",
    "for ss in sss:\n",
    "    with open('../pseudo/'+ss+'.tgt.datalabel') as f:\n",
    "        ff = open('../pseudo/'+ss+'.tgt.datapos', 'r')\n",
    "        tof = open('../pseudo/'+ss+'.tgt.dataptr', 'w')\n",
    "\n",
    "        for yxj, (label, pos) in enumerate(zip(f, ff)):\n",
    "            pos = [int(tt) for t in pos.strip().split('\\t') for tt in t.strip().split(' ') if tt]\n",
    "            tolist = []\n",
    "            nownum = 0\n",
    "            labellist = label.strip().split(' ')\n",
    "            labelnum = [-1] * len(labellist)\n",
    "            for i, n in enumerate(labellist):\n",
    "                if n == '1':\n",
    "                    labelnum[i] = nownum                    \n",
    "                    nownum += 1\n",
    "            for i in range(0, len(pos), 2):\n",
    "                try:\n",
    "                    tolist.append(str(labelnum[pos[i]]))\n",
    "                except IndexError:\n",
    "                    print(yxj)\n",
    "                    print(pos)\n",
    "                    print(ss)\n",
    "                    print(labellist)\n",
    "                    print(labelnum)\n",
    "                    raise\n",
    "            tof.write(' '.join(tolist)+'\\n')\n",
    "        tof.close()"
   ]
  },
  {
   "source": [
    "# check num"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = ['train', 'dev', 'test']\n",
    "for ss in sss:\n",
    "    with open('../pseudo/'+ss+'.tgt.textlabel') as f:\n",
    "        f2 = open('../pseudo/'+ss+'.tgt.dataptr')\n",
    "        for i, (label, ptr) in enumerate(zip(f, f2)):\n",
    "            if len([x for x in ptr.strip().split(' ') if x]) != label.split(' ').count('1'):\n",
    "                print(ptr.strip().split(' '), label.split(' ').count('1'))\n",
    "                print(i, ss)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "[1, 2, 1, 3].count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}